Migrating to event-driven microservices requires making the necessary business domain data available in the event broker,
consumable as event streams. Doing so is a process known as data liberation, and involves sourcing the data from the existing
systems and state stores that contain it.

Data produced to an event stream can be accessed by any system, event-driven or otherwise. While event-driven applications
can use streaming frameworks and native consumers to read the events, legacy applications may not be able to access them
as easily due to a variety of factors, such as technology and performance limitations. In this case, you may need to sink
the events from an event stream into an existing state store.

### What Is Data Liberation?
Data liberation is the identification and publication of cross-domain data sets to their corresponding event streams and
is part of a migration strategy for event-driven architectures.. Cross-domain data sets include any data stored in one 
data store that is required by other external systems. Point-to-point dependencies between existing services, and data
stores often highlight the cross-domain data that should be liberated.

Data liberation enforces two primary features of event-driven architecture: the single source of truth and the elimination
of direct coupling between systems.
By serving as a single source of truth, these streams also standardize the way in which systems across the organization 
access data. Systems no longer need to couple directly to the underlying data stores and applications, but instead can 
couple solely on the data contracts of event streams. 

#### Compromises for Data Liberation
A data set and its liberated event stream must be kept fully in sync, although this requirement is limited to eventual 
consistency due to the latency of event propagation. A stream of liberated events must materialize back into an exact 
replica of the source table, and this property is used extensively for event-driven microservices.

In the perfect world, all state would be created, managed, maintained, and restored from the single source of truth of 
the event streams. Any shared state should be published to the event broker first and materialized back to any services 
that need to materialize the state, including the service that produced the data in the first place.

While the ideal of maintaining state in the event broker is accessible for new microservices and refactored legacy 
applications, it is not necessarily available or practical for all applications. This is particularly true for services 
that are unlikely to ever be refactored or changed beyond initial integration with change-data capture mechanisms.


There is an opportunity for compromise here. You can use data liberation patterns to extract the data out of the data 
store and create the necessary event streams. This is a form of unidirectional event-driven architecture, as the legacy 
system will not be reading back from the liberated event stream.

Instead, the fundamental goal is to keep the internal data set synchronized with the external event stream through 
strictly controlled publishing of event data. The event stream will be eventually consistent with the internal data set 
of the legacy application,

#### Converting Liberated Data to Events
Use the same standard format for both liberated event data and native event data across your organization.
It is extremely important to provide a reliable and up-to-date schema of the produced data and to carefully consider the
evolution of the data over time.

### Data Liberation Patterns
There are three main data liberation patterns that you can use to extract data from the underlying data store. Since 
liberated data is meant to form the new single source of truth, it follows that it must contain the entire set of data 
from the data store.

**_Query-based_**
You extract data by querying the underlying state store. This can be performed on any data store.

**_Log-based_**
You extract data by following the append-only log for changes to the underlying data structures. This option is available
only for select data stores that maintain a log of the modifications made to the data.

**_Table-based_**
In this pattern, you first push data to a table used as an output queue. Another thread or separate process queries the 
table, emits the data to the relevant event stream, and then deletes the associated entries. This method requires that 
the data store support both transactions and an output queue mechanism, usually a standalone table configured for use 
as a queue.


One method of liberating data involves the usage of a dedicated, centralized framework to extract data into event streams
Examples of centralized frameworks for capturing event streams include Kafka Connect (exclusively for the Kafka platform),
Apache Gobblin, and Apache NiFi. Each framework allows you to execute a query against the underlying data set with the
results piped through to your output event streams. Each option is also scalable, such that you can add further instances
to increase the capacity for executing change-data capture (CDC) jobs. They support various levels of integration with
the schema registry offered by Confluent (Apache Kafka), but customization can certainly be performed to support other
schema registries.

### Liberating Data by Query
Query-based data liberation involves querying the data store and emitting selected results to an associated event stream.
A client is used to request the specific data set from the data store using the appropriate API, SQL, or SQL-like language.
A data set must be bulk-queried to provide the history of events. Periodic updates then follow, ensuring that changes are
produced to the output event stream.

#### Bulk Loading
Bulk loading queries and loads all of the data from the data set. Bulks loads are performed when the entire table needs 
to be loaded at each polling interval, as well as prior to ongoing incremental updates.

Bulk loading can be expensive, as it requires obtaining the entire data set from the data store. For smaller data sets 
this tends not to be a problem, but large data sets, especially those with millions or billions of records, may be difficult
to obtain. For querying and processing very large data sets I recommend you research best practices for your particular
data store, since these can vary significantly with implementation.

#### Incremental Timestamp Loading
With incremental timestamp loading, you query and load all data since the highest timestamp of the previous query’s results.
This approach uses an updated-at column or field in the data set that keeps track of the time when the record was last 
modified. During each incremental update, only records with updated-at timestamps later than the last processed time
are queried.


#### Autoincrementing ID Loading
Autoincrementing ID loading involves querying and loading all data larger than the last value of the ID. This requires a
strictly ordered autoincrementing Integer or Long field. This approach is often used for querying tables with immutable 
records, such as when using the outbox tables

#### Custom Querying
A custom query is limited only by the client querying language. This approach is often used when the client requires only
a certain subset of data from a larger data set, or when joining and denormalizing data from multiple tables to avoid 
over-exposure of the internal data model. 

#### Incremental Updating
There must be a field that the query can use to filter out records it has already processed from those it has yet to process.
The second step is to determine the frequency of polling and the latency of the updates. Higher-frequency updates provide
lower latency for data updates downstream, though this comes at the expense of a larger total load on the data store.

#### Benefits of Query-Based Updating
**_Customizability_**
Any data store can be queried, and the entire range of client options for querying is available.

**_Independent polling periods_**
Specific queries can be executed more frequently to meet tighter SLAs (service-level agreements), while other more expensive
queries can be executed less frequently to save resources.

**_Isolation of internal data models_**
Relational databases can provide isolation from the internal data model by using views or materialized views of the 
underlying data. This technique can be used to hide domain model information that should not be exposed outside of
the data store.

#### Drawbacks of Query-Based Updating
**_Required updated-at timestamp_**
The underlying table or namespace of events to query must have a column containing their updated-at timestamp. This is 
essential for tracking the last update time of the data and for making incremental updates.

**_Untraceable hard deletions_**
Hard deletions will not show up in the query results, so tracking deletions is limited to flag-based soft deletions, such
as a boolean is_deleted column.

**_Brittle dependency between data set schema and output event schema_**
Data set schema changes may occur that are incompatible with downstream event format schema rules. Breakages are increasingly
likely if the liberation mechanism is separate from the code base of the data store application, which is usually the case
for query-based systems.

**_Intermittent capture_**
Data is synced only at polling intervals, and so a series of individual changes to the same record may only show up as a
single event.

**_Production resource consumption_**
Queries use the underlying system resources to execute, which can cause unacceptable delays on a production system. This
issue can be mitigated by the use of a read-only replica, but additional financial costs and system complexity will apply.

**_Variable query performance due to data changes_**
The quantity of data queried and returned varies depending on changes made to the underlying data. In the worst-case scenario,
the entire body of data is changed each time. This can result in race conditions when a query is not finished before the
next one starts.


### Liberating Data Using Change-Data Capture Logs
Another pattern for liberating data is using the data store’s underlying change-data capture logs (binary logs in MySQL,
write-ahead logs for PostgreSQL) as the source of information. This is an append-only data logging structure that details
everything that has happened to the tracked data sets over time.

The technology options for change-data capture are narrower than those for query-based capturing. Not all data stores 
implement an immutable logging of changes, and of those that do, not all of them have off-the-shelf connectors available
for extracting the data. This approach is mostly applicable to select relational databases, such as MySQL and PostgreSQL,
though any data store with a set of comprehensive changelogs is a suitable candidate. Many other modern data stores expose
event APIs that act as a proxy for a physical write-ahead log. For example, MongoDB provides a Change Streams interface,
whereas Couchbase provides replication access via its internal replication protocol.

The data store log is unlikely to contain all changes since the beginning of time, as it can be a huge amount of data and
is usually not necessary to retain. You will need to take a snapshot of the existing data prior to starting the change-data
capture process from the data store’s log. This snapshot usually involves a large, performance-impacting query on the 
table and is commonly referred to as bootstrapping. You must ensure that there is overlap between the records in the bootstrapped
query results and the records in the log, such that you do not accidentally miss any records.

There are a number of options available for sourcing data from changelogs. Debezium is one of the most popular choices 
for relational databases, as it supports the most common ones. Debezium can produce records to both Apache Kafka and Apache
Pulsar with its existing implementations. Support for additional brokers is certainly possible, though it may require 
some in-house development work. Maxwell is another example of a binary log reader option, though it is currently limited
in support to just MySQL databases and can produce data only to Apache Kafka.

MySQL database emitting its binary changelog. A Kafka Connect service, running a Debezium connector, is consuming the raw
binary log. Debezium parses the data and converts it into discrete events. Next, an event router emits each event to a 
specific event stream in Kafka, depending on the source table of that event. Downstream consumers are now able to access
the database content by consuming the relevant event streams from Kafka.


#### Benefits of Using Data Store Logs
**_Delete tracking_**
Binary logs contain hard record deletions. These can be converted into delete events without the need for soft deletes as
in query-based updates.

**_Minimal effect on data store performance_**
For data stores that use write-ahead and binary logs, change-data capture can be performed without any impact to the data
store’s performance. For those that use change tables, such as in SQL Server, the impact is related to the volume of data.

**_Low-latency updates_**
Updates can be propagated as soon as the event is written to the binary and write-ahead logs. This results in very low 
latency when compared to other data liberation patterns.

#### Drawbacks of Using Data Base Logs
**_Exposure of internal data models_**
The internal data model is completely exposed in the changelogs. Isolation of the underlying data model must be carefully
and selectively managed, unlike query-based updating, where views can be used to provide isolation.

**_Denormalization outside of the data store_**
Changelogs contain only the event data. Some CDC mechanisms can extract from materialized views, but for many others, 
denormalization must occur outside of the data store. This may lead to the creation of highly normalized event streams, 
requiring downstream microservices to handle foreign-key joins and denormalization.

**_Brittle dependency between data set schema and output event schema_**
Much like the query-based data liberation process, the binary-log-based process exists outside of the data store application.
Valid data store changes, such as altering a data set or redefining a field type, may be completely incompatible for 
the specific evolution rules of the event schema.

### Liberating Data Using Outbox Tables
An outbox table contains notable changes made to the internal data of a data store, with each significant update stored 
as its own row. Whenever an insert, update, or delete is made to one of the data store tables marked for change-data capture
, a corresponding record can be published to the outbox table. Each table under change-data capture can have its own
outbox table, or a single outbox can be used for all changes

Both the internal table updates and the outbox updates must be bundled into a single transaction, such that each occurs 
only if the entire transaction succeeds.

The outbox table pattern leverages the durability of the data store to provide a write-ahead log for events awaiting to 
be published to external event streams.

Some databases, such as SQL Server, do not provide change-data capture logs, but instead provide change-data tables. 
These tables are often used to audit the operations of the database and come as a built-in option. External services, 
such as the aforementioned Kafka Connect and Debezium, can connect to databases that use a CDC table instead of a CDC 
log and use the query-based pattern to extract events and produce them to event streams.

The records in outbox tables must have a strict ordering identifier, for the same primary key may be updated many times 
in short order. Alternatively, you could overwrite the previous update for that primary key, though this requires finding
the previous entry first and introduces additional performance overhead. It also means that the overwritten record will 
not be emitted downstream.
An autoincrementing ID, assigned at insertion time, is best used to determine the order in which the events are to be published. 

#### Performance Considerations
The inclusion of outbox tables introduces additional load on the data store and its request-handling applications. For 
small data stores with minimal load, the overhead may go completely unnoticed. Alternately, it may be quite expensive with
very large data stores, particularly those with significant load and many tables under capture.

#### Isolating Internal Data Models
Exposing the internal data model to downstream consumers is an anti-pattern. Downstream consumers should only access data
formatted with public-facing data contracts as described
The data store client can instead denormalize data upon insertion time such that the outbox mirrors the intended public 
data contract, though this does come at the expense of additional performance and storage space. Another option is to 
maintain the 1:1 mapping of changes to output event streams and denormalize the streams with a downstream event processor
dedicated to just this task.

This is a process that I call eventification, as it converts highly normalized relational data into easy-to-consume single
event updates.

The extent to which the internal data models are isolated from external consumers tends to become a point of contention 
in organizations moving toward event-driven microservices. Isolating the internal data model is essential for ensuring 
decoupling and independence of services and to ensure that systems need only change due to new business requirements, 
and not upstream internal data-model changes.


#### Ensuring Schema Compatibility
Schema serialization (and therefore, validation) can also be built into the capture workflow. This may be performed either
before or after the event is written to the outbox table.

Serializing prior to committing the transaction to the outbox table provides the strongest guarantee of data consistency.
A serialization failure will cause the transaction to fail and roll back any changes made to the internal tables, ensuring 
that the outbox table and internal tables stay in sync.

One drawback of serializing before publishing is that performance may suffer due to the serialization overhead. This may
be inconsequential for light loads but could have more significant implications for heavier loads. You will need to ensure 
your performance needs remain met.

Before-the-fact serialization provides a stronger guarantee against incompatible data than after-the-fact and prevents 
propagation of events that violate their data contract. The tradeoff is that this implementation will also prevent the business
process from completing should serialization fail, as the transaction must be rolled back.

#### Benefits of event-production with outbox tables

**_Multilanguage support_**
This approach is supported by any client or framework that exposes transactional capabilities.

**_Before-the-fact schema enforcement_**
Schemas can be validated by serialization before being inserted into the outbox table.

**_Isolation of the internal data model_**
Data store application developers can select which fields to write to the outbox table, keeping internal fields isolated.

**_Denormalization_**
Data can be denormalized as needed before being written to the outbox table.


#### Drawbacks of event production with outbox tables

**_Required application code changes_**
The application code must be changed to enable this pattern, which requires development and testing resources from the 
application maintainers.

**_Business process performance impact_**
The performance impact to the business workflow may be nontrivial, particularly when validating schemas via serialization.
Failed transactions can also prevent business operations from proceeding.

**_Data store performance impact_**
The performance impact to the data store may be nontrivial, especially when a significant quantity of records are being
written, read, and deleted from the outbox.


Performance impacts must be balanced against other costs. For instance, some organizations simply emit events by parsing
change-data capture logs and leave it up to downstream teams to clean up the events after the fact. This incurs its own 
set of expenses in the form of computing costs for processing and standardizing the events, as well as human-labor costs
in the form of resolving incompatible schemas and attending to the effects of strong coupling to internal data models. 
Costs saved at the producer side are often dwarfed by the expenses incurred at the consumer side for dealing with these 
issues.


### The Impacts of Sinking and Sourcing on a Business
A centralized framework allows for lower-overhead processes for liberating data. This framework may be operated at scale
by a single team, which in turn supports the data liberation needs of other teams across the organization. Teams looking 
to integrate then need only concern themselves with the connector configuration and design, not with any operational duties.
This approach works best in larger organizations where data is stored in multiple data stores across multiple teams, as 
it allows for a quick start to data liberation without each team needing to construct its own solution.

There are two main traps that you can fall into when using a centralized framework. First, the data sourcing/sinking 
responsibilities are now shared between teams. The team operating the centralized framework is responsible for the stability,
scaling, and health of both the framework and each connector instance. Meanwhile, the team operating the system under 
capture is independent and may make decisions that alter the performance and stability of the connector, such as adding 
and removing fields, or changing logic that affects the volume of data being transmitted through the connector. This 
introduces a direct dependency between these two teams. These changes can break the connectors, but may be detected only 
by the connector management team, leading to linearly scaling, cross-team dependencies. This can become a difficult-to-manage
burden as the number of changes grows.

The second issue is a bit more pervasive, especially in an organization where event-driven principles are only partially 
adopted. Systems can become too reliant upon frameworks and connectors to do their event-driven work for them. Once data
has been liberated from the internal state stores and published to event streams, the organization may become complacent
about moving onward into microservices. Teams can become overly reliant upon the connector framework for sourcing and 
sinking data, and choose not to refactor their applications into native event-driven applications. In this scenario they
instead prefer to just requisition new sources and sinks as necessary, leaving their entire underlying application 
completely ignorant to events.

WARNING
CDC tools are not the final destination in moving to an event-driven architecture, but instead are primarily meant to 
help bootstrap the process. The real value of the event broker as the data communication layer is in providing a robust,
reliable, and truthful source of event data decoupled from the implementation layers, and the broker is only as good a
s the quality and reliability of its data.

Both of these issues can be mitigated through a proper understanding of the role of the change-data capture framework
. Perhaps counterintuitively, it’s important to minimize the usage of the CDC framework and have teams implement their
own change-data capture (such as the outbox pattern) despite the additional up-front work this may require. Teams become
solely responsible for publishing and their system’s events, eliminating cross-team dependencies and brittle connector-based
CDC. This minimizes the work that the CDC framework team needs to do and allows them to focus on supporting products 
that truly need it.

Reducing the reliance on the CDC framework also propagates an “event-first” mind-set. Instead of thinking of event stream
s as a way to shuffle data between monoliths, you view each system as a direct publisher and consumer of events, joinin
g in on the event-driven ecosystem. By becoming an active participant in the EDM ecosystem, you begin to think about when
and how the system needs to produce events, about the data out there instead of just the data in here. This is an important
part of the cultural shift toward successful implementation of EDM.

For products with limited resources and those under maintenance-only operation, a centralized source and sink connector
system can be a significant boon. For other products, especially those that are more complex, have significant event 
stream requirements, and are under active development, ongoing maintenance and support of connectors is unsustainable.
In these circumstances it is best to schedule time to refactor the codebase as necessary to allow the application to 
become a truly native event-driven application.

Finally, carefully consider the tradeoffs of each of the CDC strategies. This often becomes an area of discussion and 
contention within an organization, as teams try to figure out their new responsibilities and boundaries in regard to 
producing their events as the single source of truth. Moving to an event-driven architecture requires investment into 
the data communication layer, and the usefulness of this layer can only ever be as good as the quality of data within 
it. Everyone within the organization must shift their thinking to consider the impacts of their liberated data on the
rest of the organization and come up with clear service-level agreements as to the schemas, data models, ordering,
latency, and correctness for the events they are producing.
